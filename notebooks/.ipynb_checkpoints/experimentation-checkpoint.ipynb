{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Image\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizerFast, RobertaTokenizerFast, TFBertForMaskedLM, TFRobertaForMaskedLM\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 326 stopwords from SPACY\n",
      "----------------------------------\n",
      "382 combined stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOPWORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "SKLEARN_STOPWORDS = set(SKLEARN_STOPWORDS)\n",
    "nltk.download('stopwords')\n",
    "# NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "# print(f'Loaded {len(NLTK_STOPWORDS)} stopwords from NLTK')\n",
    "print(f'Loaded {len(SPACY_STOPWORDS)} stopwords from SPACY')\n",
    "# print(f'Loaded {len(SKLEARN_STOPWORDS)} stopwords from SKLEARN')\n",
    "stop_words = list(set.union(*[SPACY_STOPWORDS, NLTK_STOPWORDS]))\n",
    "print('----------------------------------')\n",
    "print(f'{len(stop_words)} combined stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'fire' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_mlm(sentence, top_k=10):\n",
    "    sentence_tensor = tokenizer(sentence, return_tensors='tf')['input_ids']\n",
    "    mask_indices = tf.where(sentence_tensor==tokenizer.mask_token_id)\n",
    "    predictions = tf.squeeze(model(sentence_tensor)[0])\n",
    "    expansions = dict()\n",
    "    for i, (_, ix) in enumerate(mask_indices):\n",
    "        top_k_indices = tf.math.top_k(predictions[ix], top_k).indices\n",
    "        tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "        words = [tokenizer.convert_tokens_to_string(token).strip() for token in tokens]\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word for word in words if (word.isalpha() and not word in stop_words)]\n",
    "        expansions[f'mask_{i}'] = words\n",
    "        return Counter(list(chain(*expansions.values())))\n",
    "def generate_keywords(query, expansion, top_k=5):\n",
    "    results =  {k:expand_mlm(v, top_k=top_k) for k,v in expansion(query).items()}\n",
    "    results['positive'] = reduce(lambda a,b: a+b, [v for k,v in results.items() if k.startswith('positive')])\n",
    "    results['negative'] = reduce(lambda a,b: a+b, [v for k,v in results.items() if k.startswith('negative')])\n",
    "    results['neutral'] = reduce(lambda a,b: a+b, [v for k,v in results.items() if k.startswith('neutral')])\n",
    "    results['combined'] = reduce(lambda a,b: a+b, results.values())\n",
    "    return {k:dict(v) for k,v in results.items()}\n",
    "#     templates = list(chain(*[input_context_pro, input_context_con, input_context_neutral]))\n",
    "#     counters = [expand_mlm(template, top_k=top_k) for template in templates]\n",
    "#     weighted_keywords = dict(reduce(lambda x,y: x+y, counters))\n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'myth': 1,\n",
       "         'joke': 1,\n",
       "         'bird': 1,\n",
       "         'robot': 1,\n",
       "         'friend': 1,\n",
       "         'dog': 1,\n",
       "         'cat': 1,\n",
       "         'spider': 1,\n",
       "         'meme': 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_mlm('spiderman is a <mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sampler_input(sentence, pre='', post='', pre_mask=0, post_mask=5, mask_token='[MASK]',cap=False, low=False):\n",
    "    if cap:\n",
    "        sentence = sentence.capitalize()\n",
    "    if low:\n",
    "        sentence = sentence.lower()\n",
    "    return f\"{pre_mask*(mask_token+' ')}&{pre}{sentence}{post}&{post_mask*(' '+mask_token)}.\"\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "        tokenized_tensor = tokenizer(sentence, return_tensors='tf')['input_ids']\n",
    "        tokens = tf.squeeze(tokenized_tensor)\n",
    "        return tokens\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "    \n",
    "def sample_from_logits(logits, num_samples=10):\n",
    "    categorical = tfd.Categorical(logits=logits)\n",
    "    return categorical.sample(num_samples)\n",
    "\n",
    "def temperature(logits, temp=1.0):\n",
    "    return logits / temp\n",
    "\n",
    "def top_k_filter(logits, top_k=100):\n",
    "    indices_to_filter = tf.math.top_k(-logits, k=len(logits)-top_k).indices #TODO CHECK IF THIS SUBSTRACTION IS CORRECT\n",
    "    indices_to_filter = tf.expand_dims(indices_to_filter, axis=1)\n",
    "    filters = tf.fill([len(logits)-top_k],-np.inf)\n",
    "    filtered_logits = tf.tensor_scatter_nd_update(logits, indices_to_filter, filters)\n",
    "    return filtered_logits\n",
    "\n",
    "def top_p_filter(logits, top_p=0.8):\n",
    "    logits = tf.squeeze(logits)\n",
    "    indices_sorted = tf.argsort(logits, direction=\"DESCENDING\")\n",
    "    logits_sorted = tf.gather(logits, indices_sorted)\n",
    "    probs_sorted = tf.nn.softmax(logits_sorted, axis=-1)\n",
    "#     cat = tfd.Categorical(logits=logits) #TODO CHECK IF USING TF PROBABILITY IS SLOWER\n",
    "#     probs = cat.prob(list(range(len(logits))))\n",
    "#     indices_sorted = tf.argsort(probs, direction='DESCENDING')\n",
    "#     probs_sorted = tf.gather(probs, indices_sorted)\n",
    "    cutoff_index = tf.argmax(tf.cumsum(probs_sorted)>top_p)\n",
    "    indices_to_filter = indices_sorted[cutoff_index+1:]\n",
    "    indices_to_filter = tf.expand_dims(indices_to_filter, axis=1)\n",
    "    filters = tf.fill([len(indices_to_filter)],-np.inf)\n",
    "    filtered_logits = tf.tensor_scatter_nd_update(logits, indices_to_filter, filters)    \n",
    "    return filtered_logits\n",
    "\n",
    "def return_nucleus_size(logits, top_p=0.8, print_greedy=False):\n",
    "    logits = tf.squeeze(logits)\n",
    "    indices_sorted = tf.argsort(logits, direction=\"DESCENDING\")\n",
    "    logits_sorted = tf.gather(logits, indices_sorted)\n",
    "    probs_sorted = tf.nn.softmax(logits_sorted, axis=-1)\n",
    "    cumulative_probs_sorted = tf.math.cumsum(probs_sorted, axis=-1)\n",
    "    if print_greedy:\n",
    "        print(tokenizer.decode(indices_sorted[0]))\n",
    "#     cutoff_index = tf.argmax(cumulative_probs_sorted>top_p)\n",
    "#     indices_to_filter = indices_sorted[cutoff_index+1:]\n",
    "#     indices_to_filter = tf.expand_dims(indices_to_filter, axis=1)\n",
    "#     filters = tf.fill([len(indices_to_filter)],-np.inf)\n",
    "#     filtered_logits = tf.tensor_scatter_nd_update(logits, indices_to_filter, filters)    \n",
    "    return 1 + tf.argmax(cumulative_probs_sorted>top_p).numpy()\n",
    "\n",
    "def frozen_indices(sentence, separator=\"&\"):\n",
    "    indices = [(np.array(match.span()) - i) for i,match in enumerate(re.finditer(separator, sentence))]\n",
    "    indices = [np.concatenate(thing)[[0,-1]] for thing in list(pairwise(indices))]\n",
    "    indices = [(thing - np.array((0,1))) for thing in indices]\n",
    "    mapping = tokenizer(sentence.replace(\"&\", \"\"), return_offsets_mapping=True, return_attention_mask=False, return_token_type_ids=False)['offset_mapping'][1:-1]\n",
    "    tokenized = tokenizer.tokenize(sentence.replace(\"&\", \"\"))\n",
    "    r = len(tokenized)+1\n",
    "    mapping = dict(zip(mapping, list(range(1,r))))\n",
    "    all_ix = {}\n",
    "    for span in indices:\n",
    "        a,b = span\n",
    "        try:\n",
    "            i,j = {k:v for k,v in mapping.items() if k[0]==a or k[1]==b}.values()\n",
    "            ix_span = list(range(i, j+1))\n",
    "        except:\n",
    "            ix_span = {k:v for k,v in mapping.items() if k[0]==a or k[1]==b}.values()\n",
    "        all_ix.update({i:tokenized[i-1] for i in ix_span})\n",
    "    return set(all_ix.keys()), sentence.replace(\"&\", \"\")\n",
    "\n",
    "def gibbs_one_iteration(tokens, frozen={}, randomized=True, top_k=0, top_p=1.0, temp=1.0):\n",
    "    ix = list(range(1,len(tokens)-1))\n",
    "    ix = list(set(ix)-frozen)\n",
    "    if randomized:\n",
    "        np.random.shuffle(ix)\n",
    "    mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "#     token_tensors = []\n",
    "    for i in ix:\n",
    "        indices = tf.constant([[i]])\n",
    "        updates = mask_token\n",
    "        masked = tf.tensor_scatter_nd_update(tokens, indices, updates) #FEED THIS INTO BERT\n",
    "        \n",
    "        masked_token_logits = tf.squeeze(model(tf.expand_dims(masked, axis=0))[0])[i] #LOGITS FOR MASKED TOKEN\n",
    "        \n",
    "        if temp!=1.0:\n",
    "            masked_token_logits = temperature(masked_token_logits, temp=temp) #TEMPERATURE ADJUSTMENT. MORE THAN 1 MAKES THE MODEL LESS CONFIDENT AND MORE LIKELY TO SAMPLE LESS LIKELY TOKENS\n",
    "        \n",
    "        if top_k>0:\n",
    "            masked_token_logits = top_k_filter(masked_token_logits, top_k=top_k)\n",
    "        if top_p<1.0:\n",
    "            masked_token_logits = top_p_filter(masked_token_logits, top_p=top_p)\n",
    "        \n",
    "        replacement = sample_from_logits(masked_token_logits, num_samples=1) # test to see if you have to sample, or get the value from the input sentence\n",
    "        \n",
    "        tokens = tf.tensor_scatter_nd_update(tokens, indices, replacement)\n",
    "    \n",
    "    new_sentence = tokenizer.decode(tokens)\n",
    "    return new_sentence, tokens\n",
    "\n",
    "\n",
    "def gibbs_sampler(sentence, num_iterations=10, top_k=0, top_p=1.0, temp=1.0, print_generations=True):\n",
    "    frozen, sentence = frozen_indices(sentence)\n",
    "    new_tokens = tokenize_sentence(sentence)\n",
    "    sentences = []\n",
    "    for i in range(num_iterations):\n",
    "        new_sentence, new_tokens = gibbs_one_iteration(tokens=new_tokens, frozen=frozen, top_p=top_p, temp=temp)\n",
    "        print(new_tokens)\n",
    "        if print_generations:\n",
    "            print(new_sentence)\n",
    "            print(\"-----------------\")\n",
    "        sentences.append(new_sentence)            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"&Spiderman& is a <mask> <mask> <mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([    0 41624   397    11     5    78   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the first place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n",
      "tf.Tensor([    0 41624   397    11     5  1593   317     4     2], shape=(9,), dtype=int32)\n",
      "<s>Spiderman in the wrong place.</s>\n",
      "-----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>Spiderman in the first place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>',\n",
       " '<s>Spiderman in the wrong place.</s>']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gibbs_sampler(sentence, top_p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs_one_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen, sentence = frozen_indices(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spiderman is a <mask> <mask> <mask>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Spiderman as a young boy.</s>'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(11,), dtype=int32, numpy=\n",
       "array([    0,     0, 41624,   397,    16,    10, 50264, 50264, 50264,\n",
       "           2,     2], dtype=int32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = tokenize_sentence(sentence)\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence, new_tokens = gibbs_one_iteration(tokens=new_tokens, frozen=frozen, top_p=0.8, temp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Spiderman as a young boy.</s>'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int32, numpy=\n",
       "array([    0, 41624,   397,    25,    10,   664,  2143,     4,     2],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_potential(sentence):\n",
    "    tokenized_tensor = tokenizer(sentence, return_tensors='tf')['input_ids']\n",
    "    tokens = tf.squeeze(tokenized_tensor)\n",
    "    ix = list(range(1,len(tokens)-1))\n",
    "    mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "    log_probs = tf.constant([0.])\n",
    "    for i in ix:\n",
    "        indices = tf.constant([[i]])\n",
    "        updates = mask_token\n",
    "        masked = tf.tensor_scatter_nd_update(tokens, indices, updates) #FEED THIS INTO BERT\n",
    "        \n",
    "        masked_token_logits = tf.squeeze(model(tf.expand_dims(masked, axis=0))[0])[i] #LOGITS FOR MASKED TOKEN        \n",
    "        categorical = tfd.Categorical(logits=masked_token_logits)\n",
    "        log_prob = categorical.log_prob(tokens[i])\n",
    "        log_prob = tf.expand_dims(log_prob, axis=0)\n",
    "        log_probs = tf.concat([log_probs, log_prob], axis=0)\n",
    "#         log_probs.append(categorical.log_prob(tokens[i]).numpy())        \n",
    "    log_probs = np.array(log_probs)\n",
    "#     return np.sum(log_probs)/len(log_probs)\n",
    "    result = tf.reduce_sum(log_probs)/len(log_probs)\n",
    "    return result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><s>I once saw a man so tall I could not look into his eyes. I could not let a stranger even look at me.</s></s>'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0, 0, 100, 683, 794, 10, 313, 98, 6764, 38, 115, 45, 356, 88, 39, 2473, 4, 38, 115, 45, 905, 10, 12443, 190, 356, 23, 162, 4, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1/ sentence_log_potential(\"Chris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1/ sentence_log_potential(\"Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_sampler_input('test is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"&I once saw a man so tall I could not look into his eyes.& <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask>.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'cls_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-51600a5e5081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<s>I once saw a man so tall I could not look into his eyes. I could not let a stranger even look at me.</s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2287\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2288\u001b[0m             )\n\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m         )\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         )\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         )\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'cls_token'"
     ]
    }
   ],
   "source": [
    "tokenizer(\"<s>I once saw a man so tall I could not look into his eyes. I could not let a stranger even look at me.</s>\", cls_token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
       "array([    0,     0,   100,   683,   794,    10,   313,    98,  6764,\n",
       "          38,   115,    45,   356,    88,    39,  2473,     4,    38,\n",
       "         115,    45,   905,    10, 12443,   190,   356,    23,   162,\n",
       "           4,     2,     2], dtype=int32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence(\"<s>I once saw a man so tall I could not look into his eyes. I could not let a stranger even look at me.</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I once saw a man so tall I could not look into his eyes. I could not let a stranger even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let this man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I will not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let that man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I did not let that man ever look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let the man even look at me.</s>\n",
      "-----------------\n",
      "<s>I once saw a man so tall I could not look into his eyes. I would not let the man even look at me.</s>\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sentences = gibbs_sampler(sentence, num_iterations=50, top_p=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 0, 100, 683, 794, 10, 313, 98, 6764, 38, 115, 45, 356, 88, 39, 2473, 4, 38, 115, 45, 905, 10, 12443, 190, 356, 23, 162, 4, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<s>I once saw a man so tall I could not look into his eyes. I could not let a stranger even look at me.</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"&Spiderman& &is& &a& [MASK] [MASK] [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = gibbs_sampler(sentence, num_iterations=100, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var = model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dir(model.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
